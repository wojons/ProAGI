## Goal: Enhance Metric Collection within the Sandbox

Improve the metric collection in `@sandbox_images/python_runner/sandbox_server.py` to capture more detailed and accurate metrics, particularly LLM token counts and MCP call timings/counts.

**Context:**
*   Sandbox Server Code: `@sandbox_images/python_runner/sandbox_server.py`
*   MCP Client: `MCPClient` class within the sandbox server.
*   LLM Handler: `_run_llm_prompt` function.
*   Target Response Structure: `ExecuteResponse` model including the `metrics` dictionary.

**Plan:**

1.  **Read Sandbox Server Code:** Use `read_file` to load `@sandbox_images/python_runner/sandbox_server.py`.
2.  **Modify `MCPClient.call_tool`:**
    *   Wrap the `httpx.AsyncClient().post(...)` call with timing logic (`start_time = time.monotonic()`, `duration_ms = (time.monotonic() - start_time) * 1000`).
    *   Modify the method to return not just the `data`, but also a dictionary containing call metrics, e.g., `{ "data": response_data["data"], "metrics": {"duration_ms": duration_ms} }`. Handle errors appropriately.
3.  **Modify `_run_llm_prompt`:**
    *   Update the call to `mcp_client.call_tool("core.llm.generate", ...)` to expect the new return format (`result = await mcp_client.call_tool(...)`, `llm_data = result['data']`, `mcp_metrics = result['metrics']`).
    *   **Extract Token Counts:** Assume the `core.llm.generate` MCP tool's response (`llm_data`) includes token information (e.g., `llm_data['usage']['prompt_tokens']`, `llm_data['usage']['completion_tokens']`). Extract these values. *Add a TODO/assumption comment that the MCP tool provides this.*
    *   **Aggregate Metrics:** In the `metrics` dictionary returned by `_run_llm_prompt`, include:
        *   Overall execution time (`handler_duration_ms`).
        *   MCP call duration (`mcp_call_duration_ms` from `mcp_metrics`).
        *   LLM prompt tokens (`llm_prompt_tokens`).
        *   LLM completion tokens (`llm_completion_tokens`).
        *   Total LLM tokens (`llm_total_tokens`).
4.  **Modify `_run_jit_script` & `_execute_workflow`:**
    *   Update these functions similarly to handle the new `MCPClient.call_tool` return format.
    *   Aggregate MCP call timings if multiple calls are made (especially in the workflow loop). Keep track of the *number* of MCP calls made.
    *   Return aggregated metrics (`handler_duration_ms`, `total_mcp_duration_ms`, `mcp_calls_count`) in their respective `metrics` dictionaries.
5.  **Update Main `execute` Handler:** Ensure it correctly extracts the more detailed `metrics` dictionaries returned by the helper functions and includes them in the final `ExecuteResponse`.
6.  **Add Imports:** Ensure `time` module is imported.
7.  **Save Changes:** Use `write_to_file` to save the updated `@sandbox_images/python_runner/sandbox_server.py`.

**Expected Outcome:**
*   The sandbox server now collects and returns more detailed metrics in its `/execute` response, including MCP call timings/counts and LLM token usage (assuming the `core.llm.generate` tool provides token info).