## Goal: Integrate Metric Pushing to `MetricCollector`

Modify key framework components (`RequestRouter`, `SandboxManager`) and the Sandbox server (`sandbox_server.py`) to push relevant operational metrics to the `MetricCollector` service via its internal API.

**Context:**
*   Metric Collector: `@core/metrics/metric_collector.py` (contains `record_metric` method or `/record` endpoint logic)
*   Metric Schema: `@schemas/metric_record.schema.json` (defines the structure to push)
*   Components to Instrument:
    *   `@core/routing/request_router.py`
    *   `@core/sandbox/sandbox_manager.py`
    *   `@sandbox_images/python_runner/sandbox_server.py`
*   Project Standards: `@.clinerules`

**Plan:**

1.  **Review Metric Schema:** Examine `@schemas/metric_record.schema.json` to confirm required fields (`metric_name`, `metric_type`, `value`, optional `labels`, `help_text`).
2.  **Instrument `RequestRouter`:**
    *   Use `read_file` to load `@core/routing/request_router.py`.
    *   Ensure `MetricCollectorInterface` is injected in `__init__`.
    *   In `handle_incoming_request`:
        *   Before returning the final response (in `finally` block or just before return), calculate total duration (`time.monotonic() - start_time`).
        *   Prepare metric data for request count: `{ "metric_name": "uhlp_framework_request_total", "metric_type": "counter", "value": 1, "labels": {"appId": ..., "source": ..., "http_status": ...} }`.
        *   Prepare metric data for request duration: `{ "metric_name": "uhlp_framework_request_duration_ms", "metric_type": "histogram", "value": duration_ms, "labels": {...} }`.
        *   Call `await self.metric_collector.record_metric(...)` for each metric within a `try...except` block (log errors, don't fail request).
    *   Use `write_to_file` to save changes.
3.  **Instrument `SandboxManager`:**
    *   Use `read_file` to load `@core/sandbox/sandbox_manager.py`.
    *   Inject `MetricCollectorInterface` in `__init__`.
    *   In `_start_sandbox_instance`: Record a counter metric `uhlp_sandbox_start_total` on success/failure. Record a gauge metric `uhlp_sandbox_running_instances` (increment).
    *   In `_stop_sandbox_instance` / `_remove_sandbox_instance`: Record a counter metric `uhlp_sandbox_stop_total`. Record gauge `uhlp_sandbox_running_instances` (decrement).
    *   In `allocate_sandbox`: Record counter `uhlp_sandbox_allocation_total` (labels: status=success/failure).
    *   Use `await self.metric_collector.record_metric(...)`.
    *   Use `write_to_file` to save changes.
4.  **Instrument Sandbox Server (`sandbox_server.py`):**
    *   Use `read_file` to load `@sandbox_images/python_runner/sandbox_server.py`.
    *   In the main `execute` function:
        *   Retrieve sandbox metrics (execution time, MCP calls, LLM tokens) from `handler_metrics` dictionary returned by helper functions (`_run_jit_script`, `_run_llm_prompt`, `_execute_workflow`).
        *   Construct appropriate metric payloads matching the schema for these metrics (e.g., `uhlp_sandbox_execution_duration_ms`, `uhlp_sandbox_mcp_calls_total`, `uhlp_sandbox_llm_tokens_total`). **Crucially, include `appId`, `componentId`, `handlerType` in labels.**
        *   Push these metrics back to the Core Framework by including them in the `metrics` field of the `ExecuteResponse` object. (The `RequestRouter` will then forward these to the `MetricCollector`).
    *   Use `write_to_file` to save changes.
5.  **Add Basic Tests (Optional but Recommended):** Update unit tests for the modified components to include assertions that `record_metric` is called with expected data (using `AsyncMock.assert_called_with`).

**Expected Outcome:**
*   Relevant components are updated to push key operational metrics to the `MetricCollector`.
*   The sandbox server includes collected internal metrics in its `/execute` response.