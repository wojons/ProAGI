## Goal: Implement LLM Task Execution Logic in Sandbox

Add the logic to the `/execute` handler in the sandbox server to handle `handlerType: LLM` requests: fetch the prompt template via MCP, render it using Jinja2, call the `core.llm.generate` MCP tool, and return the result.

**Context:**
*   Server File: `@sandbox_images/python_runner/sandbox_server.py`
*   MCP Client: `MCPClient` class within the server file.
*   Design Docs: Section 8 [9, 23] (Prompt Templates), Section 2.4 [15] (Sandbox API), `02-sandbox-api.md` [5].
*   Dependencies: `Jinja2` from `@sandbox_images/python_runner/requirements.txt`.

**Plan:**

1.  **Read Server File:** Use `read_file` to get the content of `@sandbox_images/python_runner/sandbox_server.py`.
2.  **Locate LLM Handler Block:** Find the `elif request.context.handlerType == "LLM":` block within the `execute` function.
3.  **Implement LLM Logic:** Replace the placeholder logic within the LLM block with the following:
    *   **Extract Task Details:** Get `template_path = request.context.taskDetails.get("promptTemplate")`. Validate it exists. Get optional overrides like `model = request.context.taskDetails.get("model")`.
    *   **Fetch Prompt Template Content:**
        *   Use `mcp_client.call_tool` to call `core.state.getDefinitionFileContent` with `params={"app_id": request.context.appId, "file_path": template_path}`.
        *   Handle potential errors from MCP (e.g., file not found).
        *   Decode the base64 content and parse the YAML (Need to add `PyYAML` dependency if parsing YAML metadata like description/parameters from the template file itself). For V0.1, assume the fetched content *is* the Jinja2 template string directly. Decode the bytes to string.
    *   **Prepare Template Context:** Create a dictionary `template_context` containing data accessible to the template (e.g., `{"requestData": request.requestData, "context": request.context.dict() }`. Be mindful of security - don't pass sensitive internal context). Reference `06-prompt-template-yaml.md` [9, 23] for Jinja2 usage.
    *   **Render Prompt:** Use `jinja2.Template(template_string).render(template_context)` to generate the final prompt string. Handle rendering errors.
    *   **Prepare MCP Call:**
        *   Construct the `params` for the `core.llm.generate` tool, including the `prompt` (the rendered string), optional `model` override, and potentially other parameters passed via `request.context.taskDetails` or defined in the template YAML metadata (if parsed).
    *   **Call LLM via MCP:**
        *   Use `mcp_client.call_tool` to call `core.llm.generate` with the prepared `params` and current `request.context`.
        *   Handle potential errors from the MCP call (e.g., LLM errors propagated by the MCP server). The result data from MCP should be the LLM completion (or structured data if requested and parsed by the MCP tool).
    *   **Prepare Success Response:** Construct the `ExecuteResponse` with `resultType="success"` and the `data` received from the `core.llm.generate` call. Add relevant LLM metrics (e.g., token counts, latency) from the MCP response to the `ExecuteResponse` metrics field if available.
4.  **Add Imports:** Ensure `jinja2`, `yaml` (if parsing YAML) are imported. Add `PyYAML` to `requirements.txt` if needed.
5.  **Save Changes:** Use `write_to_file` to save the modified content back to `@sandbox_images/python_runner/sandbox_server.py`.
6.  **(Self-Correction):** If adding `PyYAML`, update Prompt 3.1 to include it in `requirements.txt`.

**Expected Outcome:**
*   The LLM handler block in `@sandbox_images/python_runner/sandbox_server.py` is implemented to fetch a prompt template via MCP, render it with Jinja2, call the `core.llm.generate` MCP tool, and return the LLM's response.
*   `sandbox_images/python_runner/requirements.txt` might be updated if `PyYAML` was added.