## Goal: Implement the `MetricCollector` Service

Define and implement the `MetricCollector` service responsible for receiving metrics from other components and exposing them in Prometheus format. Based loosely on Section 3.5 [16] (needs detailed definition).

**Context:**
*   Project Structure: `@core/`, `@schemas/`
*   Design Docs: Section 3.5 [16] (basic mention), `01-python-standards.md` [4] (logging/async)
*   Project Standards: `@.clinerules`

**Plan:**

1.  **Add Dependencies:** Use `execute_command` to add `prometheus-client` (official Python client) and potentially `aiohttp` or `FastAPI` (if building a standalone server) to `requirements.txt` and install.
2.  **Define Metric Data Schema:** Use `write_to_file` to create `schemas/metric_record.schema.json`. Define a JSON schema for the payload sent to the collector (e.g., `POST /record`). It should include fields like:
    *   `metric_name`: String (required)
    *   `metric_type`: Enum ["counter", "gauge", "histogram", "summary"] (required)
    *   `value`: Number (required for counter, gauge)
    *   `labels`: Object (optional, key-value pairs for Prometheus labels like `appId`, `componentId`, `poolName`, `stepId`, `statusCode`, etc.)
    *   `buckets`: Array of numbers (optional, required for histogram if not using defaults)
3.  **Define Interface (Optional):** Use `write_to_file` to create `core/interfaces/metric_collector_interface.py`. Define an ABC with an `async def record_metric(self, metric_data: dict)` method.
4.  **Create Implementation File:** Use `write_to_file` to create `core/metrics/metric_collector.py`.
5.  **Implement `MetricCollector` Class:**
    *   Create `MetricCollector` (inheriting from interface if defined).
    *   **Initialization (`__init__`)**:
        *   Initialize the Prometheus registry (`prometheus_client.CollectorRegistry`).
        *   Initialize internal dictionaries to hold defined Prometheus metric objects (`Gauge`, `Counter`, etc.) keyed by name, to avoid redefining them on each call. Use `prometheus_client.Gauge`, `Counter`, `Histogram`, `Summary`.
        *   Initialize a logger.
    *   **Implement `record_metric` Method (`async`)**:
        *   Validate input `metric_data` against `schemas/metric_record.schema.json`.
        *   Extract metric details (`name`, `type`, `value`, `labels`).
        *   Check if the metric object (`name` + `labels` keys) exists in the internal dictionary. If not, create it using the appropriate `prometheus_client` type (e.g., `Gauge(name, 'Description', labelnames=labels.keys(), registry=self.registry)`) and store it. Handle label consistency issues.
        *   Update the metric value: Use `.inc()`, `.set()`, `.observe()` depending on the `type`. Extract labels *values* from input `labels` dict, ensuring they match the `labelnames` defined when creating the metric.
        *   Log the recorded metric (optional, consider sampling if high volume).
    *   **Implement Prometheus Exposition Logic:**
        *   Add a method `generate_latest_metrics(self) -> bytes` that calls `prometheus_client.generate_latest(self.registry)` and returns the encoded metrics data.
6.  **Integrate with Web Server (Option 1: Part of Core Framework):**
    *   Modify the main core framework server (e.g., `@core/routing/request_router.py` or a main server file) to:
        *   Instantiate `MetricCollector`.
        *   Expose a `POST /record` endpoint that calls `metric_collector.record_metric`.
        *   Expose a `GET /metrics` endpoint that calls `metric_collector.generate_latest_metrics` and returns the Prometheus data with the correct content type.
7.  **Integrate with Web Server (Option 2: Standalone Server):**
    *   Create a simple `aiohttp` or `FastAPI` server in `core/metrics/server.py` that hosts the `MetricCollector` and exposes `/record` and `/metrics`. Set it up to be run as a separate process/container.
8.  **Add Basic Unit Tests:** Use `write_to_file` to create `tests/core/metrics/test_metric_collector.py`.
    *   Use `pytest`. Mock `prometheus-client` if needed, or test by checking the output of `generate_latest_metrics`.
    *   Test recording different metric types, handling labels, and error conditions.
9.  **Instrumentation Points (Informational - separate prompt needed for implementation):** Note that code needs to be added in other components (`RequestRouter`, `SandboxManager`, `CoreMCPServer`, `sandbox_server.py`) to actually *call* the `/record` endpoint or `record_metric` method. Example metrics: request count, request latency, sandbox allocation time, MCP call latency, LLM token usage, JIT execution time.

**Expected Outcome:**
*   `schemas/metric_record.schema.json` defining the metric payload.
*   `core/metrics/metric_collector.py` implementing the collector logic using `prometheus-client`.
*   Integration into a web server (either core or standalone) exposing `/record` and `/metrics`.
*   Basic unit tests in `tests/core/metrics/test_metric_collector.py`.
*   Updated `requirements.txt`.