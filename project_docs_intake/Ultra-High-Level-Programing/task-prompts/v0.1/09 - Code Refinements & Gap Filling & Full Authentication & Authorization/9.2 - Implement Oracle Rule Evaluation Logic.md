# Task 9.2: Implement Oracle Rule Evaluation Logic

**Goal:** Implement the rule evaluation logic within the `_should_optimize` method of `@core/optimization/optimization_oracle.py`.

**Context:**
- Oracle: `@core/optimization/optimization_oracle.py`
- Metric Collector Interface: `@core/interfaces/metric_collector_interface.py` (method `query_aggregate` from Task 9.1)
- Data Models: `@core/shared/data_models.py` (esp. `OptimizationConfig`, `OptimizationRule`, `MetricCondition`, `ComparisonOperator`)

**Plan:**

1.  **Analyze:** Read the `_should_optimize` method in `@core/optimization/optimization_oracle.py`. Review the structure of `OptimizationRule` and `MetricCondition`.
2.  **Implement Rule Iteration:** Modify `_should_optimize` to:
    *   Retrieve the applicable rules from `opt_config.rules`. (Handle component-specific overrides later if needed).
    *   Iterate through each `rule` in the list.
    *   For each `rule`, assume it triggers optimization (`rule_triggered = True`) initially.
3.  **Implement Condition Evaluation:** Inside the rule loop:
    *   Iterate through each `condition` in `rule.conditions`.
    *   Extract `metric_name`, `labels`, `aggregation`, `window`, `threshold`, `operator` from the `condition`.
    *   Call `actual_value = await self.metric_collector.query_aggregate(...)` using these parameters.
    *   If `actual_value` is `None` (metric unavailable), log a warning and consider the condition *not* met (`rule_triggered = False; break`).
    *   Compare `actual_value` with `condition.threshold` using the `condition.operator`. Requires mapping the `ComparisonOperator` enum value (e.g., `'>'`, `'=='`) to a Python comparison function (`>`, `==`). Handle potential type errors during comparison.
    *   If the comparison is `False`, the rule is not met (`rule_triggered = False; break`).
4.  **Determine Outcome:** After checking all conditions for a rule:
    *   If `rule_triggered` is still `True`, then this rule's conditions are met. Log the successful rule match and `return True` from `_should_optimize`.
5.  **Default Outcome:** If the loop finishes without any rule triggering, `return False`.
6.  **Update Tests:** Use `write_to_file` to update `tests/core/optimization/test_optimization_oracle.py`:
    *   Refine mock `AppDefinition`s to include specific `OptimizationRule` structures.
    *   Configure `mock_metric_collector.query_aggregate` to return specific values.
    *   Add tests verifying: rule triggers when all conditions met, rule doesn't trigger if one condition fails, rule doesn't trigger if metric is unavailable.

**Expected Outcome:** The `_should_optimize` method implements logic to evaluate multiple conditions based on (mocked) aggregated metrics according to the defined rules.